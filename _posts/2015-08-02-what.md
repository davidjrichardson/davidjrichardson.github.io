---
title: "what is it"
bg: purple
color: white
fa-icon: question
---

Chiron is a new data intensive compute cluster, designed using lessons learned from HPC:

* Scale out not up,
* Network a cluster of nodes,
* Distribute tasks to where data is available.

The cluster is built upon the MapR Hadoop distribution as well as Apache Storm and SLURM. The system also utilises Apache Spark, YARN, HDFS, and HBase for the capability to perform both **bulk/offline** and **streaming/online** anayltics. Chiron also comes with a set of hardware accelerator nodes to form a research platform into their use within data analytics.

The architecture of the cluster is heterogenous, where each disjoint set of the cluster is optimised for a specific purpose:

* Bulk/Offline:
  * Uses MapR Hadoop,
  * 28x Nodes,
  * Mix of HDDs (6TB capacity) and SSDs (3GB capacity),
  * **156TB total storage**.
* Streaming/Online:
  * Uses Apache Storm,
  * 32x 64GB RAM, 20 core nodes,
  * **Total 2TB RAM, 640 CPU cores**.
* In-memory Analytics:
  * **1x 4TB RAM, 48-core system**.
* Accelerator research platform
  * 2x Nvidia K40 12GB nodes,
  * 2x Intel Xeon Phi 16GB nodes,
  * **2x Nallatech 395 FPGA 32GB nodes**.
* All nodes use **InfiniBand interconnects** to facilitate their use as a cohesive whole.

The in-memory analytics and accelerator nodes are managed with the SLURM scheduler, allowing for a more traditional HPC approach when these nodes are used.

The original slides detailing Chiron are also available [here](/raw/chiron-slides.pdf).
